import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import load_prompt
from langchain_teddynote.prompts import load_prompt
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationSummaryBufferMemory
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_teddynote import logging
from dotenv import load_dotenv
import os
import pickle
import yaml

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. í™˜ê²½ ë³€ìˆ˜ ë° ë¡œê¹… ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# .env íŒŒì¼ì—ì„œ OPENAI_API_KEY ë“± í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
load_dotenv()

# LangSmith ë¡œê¹…: í”„ë¡œì íŠ¸ ì´ë¦„ ì§€ì • (ë¡œê·¸ ì¶”ì  ì‹œ ì‚¬ìš©)
logging.langsmith("[Project] HR RAG ì±„íŒ…ë´‡")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ìºì‹œ ë””ë ‰í† ë¦¬ ì¤€ë¹„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë””ìŠ¤í¬ì— ê²°ê³¼ë¬¼ì„ ì €ì¥í•  ìºì‹œ í´ë” ìƒì„± (í•œ ë²ˆ ìƒì„±í•˜ë©´ ì¬ì‚¬ìš©)

# 1) ìµœìƒìœ„ ìºì‹œí´ë” ì¤€ë¹„ 
if not os.path.exists(".cache"):
    os.mkdir(".cache")

# 2) embeddings ì €ì¥ìš© í´ë” ì¤€ë¹„ 
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Streamlit UI ì„¤ì • ë° ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="HR RAG ì±„íŒ…ë´‡ | HR RAG ì±„íŒ…ë´‡",
    page_icon="ğŸ’¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# SuperLawyer ìŠ¤íƒ€ì¼ CSS
st.markdown("""
<style>
    /* ì „ì²´ ë°°ê²½ */
    .main {
        background-color: #f8f9fa;
    }
    
    /* ì‚¬ì´ë“œë°” ìŠ¤íƒ€ì¼ */
    .css-1d391kg {
        background-color: #1e1e1e;
    }
    
    /* ë©”ì¸ í—¤ë” */
    .main-header {
        background: linear-gradient(90deg, #4f46e5 0%, #7c3aed 100%);
        padding: 1.5rem 2rem;
        border-radius: 12px;
        margin-bottom: 2rem;
        color: white;
        text-align: center;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
    }
    
    /* ì±„íŒ… ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    .user-message {
        background: #f3f4f6;
        padding: 1rem;
        border-radius: 12px;
        margin: 0.5rem 0;
        margin-left: 15%;
        border-left: 4px solid #6b7280;
    }
    
    .assistant-message {
        background: #eff6ff;
        padding: 1rem;
        border-radius: 12px;
        margin: 0.5rem 0;
        margin-right: 15%;
        border-left: 4px solid #3b82f6;
    }
    
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton > button {
        background: linear-gradient(90deg, #4f46e5 0%, #7c3aed 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.5rem 1rem;
        font-weight: 600;
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(79, 70, 229, 0.4);
    }
    
    /* ìƒíƒœ ë°°ì§€ */
    .status-ready {
        background: #dcfce7;
        color: #166534;
        padding: 0.25rem 0.75rem;
        border-radius: 20px;
        font-size: 12px;
        font-weight: 600;
    }
    
    .status-loading {
        background: #fef3c7;
        color: #92400e;
        padding: 0.25rem 0.75rem;
        border-radius: 20px;
        font-size: 12px;
        font-weight: 600;
    }
    
    /* ì…ë ¥ì°½ ìŠ¤íƒ€ì¼ */
    .stTextInput > div > div > input {
        border-radius: 25px;
        border: 2px solid #e5e7eb;
        padding: 0.75rem 1rem;
    }
</style>
""", unsafe_allow_html=True)

# ë©”ì¸ í—¤ë”
st.markdown("""
    <div class="main-header">
        <h1 style="margin: 0; font-size: 2rem;">âš–ï¸ HR RAG ì±„íŒ…ë´‡</h1>
        <p style="margin: 0.5rem 0 0 0; opacity: 0.9;">ë…¸ë¬´ì— í•„ìš”í•œ ì‚¬í•­ì„ ì „ë‹¬í•´ë“œë ¤ìš”</p>
    </div>
""", unsafe_allow_html=True)

# 1. ì„¸ì…˜ ìƒíƒœì— `"messages"`ë¼ëŠ” í‚¤ê°€ ì—†ìœ¼ë©´(=ìµœì´ˆ ë¡œë“œ ì‹œ) ë¹ˆ ë¦¬ìŠ¤íŠ¸ `[]`ë¥¼ í• ë‹¹
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# 2. `st.session_state`ì— `"chain"`ì´ë¼ëŠ” í‚¤ê°€ ì—†ìœ¼ë©´, `None` ê°’ì„ í• ë‹¹
if "chain" not in st.session_state:
    st.session_state["chain"] = None

# 3. retriever ì´ˆê¸°í™” ìƒíƒœ ê´€ë¦¬
if "retriever_ready" not in st.session_state:
    st.session_state["retriever_ready"] = False

# ì‚¬ì´ë“œë°” êµ¬ì„± (ì„¤ì • ì˜µì…˜)
with st.sidebar:
    st.markdown("""
        <div style="text-align: center; padding: 1rem 0; border-bottom: 1px solid #374151; margin-bottom: 1rem;">
            <h2 style="color: white; margin: 0;">âš–ï¸ HR RAG</h2>
            <p style="color: #9ca3af; margin: 0; font-size: 14px;">SuperLawyer</p>
        </div>
    """, unsafe_allow_html=True)
    
    st.header("âš™ï¸ ì„¤ì •")
    
    # ëª¨ë¸ ì„ íƒ
    selected_model = st.selectbox(
        "ğŸ¤– LLM ì„ íƒ", 
        ["gpt-3.5-turbo", "gpt-4o-mini", "gpt-4o", "claude-3-5-sonnet-20240620"], 
        index=1  # gpt-4o-miniê°€ ê¸°ë³¸ê°’
    )
    
    # ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜
    k = st.number_input(
        "ğŸ“„ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜",
        min_value=1,
        max_value=10,
        value=3
    )
    
    # ë©”ëª¨ë¦¬ ìƒíƒœ í‘œì‹œ
    if "conversation_memory" in st.session_state:
        memory = st.session_state.conversation_memory
        chat_history_data = memory.load_memory_variables({})
        chat_history = chat_history_data.get("chat_history", [])
        
        if isinstance(chat_history, list):
            num_messages = len(chat_history)
            turns = num_messages // 2
            st.markdown(f'<span class="status-ready">ğŸ’­ {turns}í„´ ëŒ€í™” ì €ì¥ì¤‘</span>', unsafe_allow_html=True)
        else:
            st.markdown('<span class="status-ready">ğŸ’­ ìš”ì•½ëœ ëŒ€í™” ì´ë ¥ ë³´ì¡´ì¤‘</span>', unsafe_allow_html=True)
    else:
        st.markdown('<span class="status-loading">ğŸ’­ ëŒ€í™” ì—†ìŒ</span>', unsafe_allow_html=True)
    
    st.divider()
    
    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    clear_btn = st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True)
    
    st.divider()
    
    # ì‹œìŠ¤í…œ ì •ë³´
    st.info("""
    **ğŸ¢ HR RAG ì±„íŒ…ë´‡**
    
    ë…¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    
    **ë°ì´í„° ê¸°ë°˜:**
    - ğŸ“š 143ê°œ HR ì „ë¬¸ ë¬¸ì„œ
    - ğŸ“„ ê¶Œê³ ì‚¬ì§, í•´ê³ , ì‹¤ì—…ê¸‰ì—¬
    - âš–ï¸ ë²•ì  ê·¼ê±° ë° ì‹¤ë¬´ ì ˆì°¨
    
    **ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ:**
    - ğŸ§  ì´ì „ ëŒ€í™” ë‚´ìš© ê¸°ì–µ
    - ğŸ“ 1000í† í° ì´ˆê³¼ì‹œ ìë™ ìš”ì•½
    - ğŸ”„ ì—°ì†ì ì¸ ë©€í‹°í„´ ëŒ€í™” ì§€ì›
    """)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ëŒ€í™” ê¸°ë¡ í—¬í¼ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì‹œì§€ ì¶œë ¥ (SuperLawyer ìŠ¤íƒ€ì¼ë¡œ í–¥ìƒ)
def print_messages():
    for msg in st.session_state["messages"]:
        if msg.role == "user":
            st.markdown(f'<div class="user-message">ğŸ‘¤ **ì‚¬ìš©ì**<br/>{msg.content}</div>', unsafe_allow_html=True)
        else:
            st.markdown(f'<div class="assistant-message">âš–ï¸ **HR ì „ë¬¸ê°€**<br/>{msg.content}</div>', unsafe_allow_html=True)

# ë©”ì‹œì§€ ì¶”ê°€ 
def add_message(role: str, content: str):
    st.session_state["messages"].append(ChatMessage(role=role, content=content))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. HR Documents â†’ FAISS Retriever ìƒì„± (ë¬´ê±°ìš´ ì‘ì—… ìºì‹œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner="HR ë¬¸ì„œë¥¼ ì²˜ë¦¬ì¤‘ì…ë‹ˆë‹¤...")
def create_retriever():
    """ê¸°ì¡´ì— ì „ì²˜ë¦¬ëœ documents.pklì—ì„œ FAISS ê²€ìƒ‰ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    try:
        # 1) ì „ì²˜ë¦¬ëœ ë¬¸ì„œ ë¡œë“œ
        documents_path = os.path.join("data", "processed", "documents.pkl")
        
        if not os.path.exists(documents_path):
            st.error(f"âŒ ë¬¸ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {documents_path}")
            return None
            
        with open(documents_path, 'rb') as f:
            documents = pickle.load(f)
        
        st.info(f"âœ… {len(documents)}ê°œì˜ HR ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
        # 2) í…ìŠ¤íŠ¸ ë¶„í•  (ì„ë² ë”© ìµœì í™”ë¥¼ ìœ„í•œ ì²­í¬ ìƒì„±)
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=500, 
            chunk_overlap=100
        )
        split_docs = splitter.split_documents(documents)
        
        st.info(f"âœ… ë¬¸ì„œë¥¼ {len(split_docs)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
        
        # 3) ì„ë² ë”© ìƒì„± (OpenAI)
        openai_api_key = os.getenv("OPENAI_API_KEY")
        
        if openai_api_key:
            embeddings = OpenAIEmbeddings()
            st.info("âœ… OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            st.error("âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        # 4) FAISS ì¸ë±ìŠ¤ ìƒì„±
        vectorstore = FAISS.from_documents(
            documents=split_docs,
            embedding=embeddings
        )
        
        # 5) ê²€ìƒ‰ê¸°(retriever)ë¡œ ë³€í™˜
        retriever = vectorstore.as_retriever(search_kwargs={"k": k})
        
        st.success("âœ… FAISS ê²€ìƒ‰ê¸° ìƒì„± ì™„ë£Œ!")
        return retriever
        
    except Exception as e:
        st.error(f"âŒ ê²€ìƒ‰ê¸° ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. LangChain RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def create_chain(retriever, model="gpt-3.5-turbo"):
    """RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    try:
        # 1) í”„ë¡¬í”„íŠ¸ ìƒì„± (YAML íŒŒì¼ì—ì„œ ë¡œë“œ)
        from langchain_core.prompts import ChatPromptTemplate
        
        # YAML íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ
        prompt_path = "src/prompt/qa_prompt.yaml"
        try:
            with open(prompt_path, 'r', encoding='utf-8') as f:
                prompt_config = yaml.safe_load(f)
                template = prompt_config['qa_template']
        except FileNotFoundError:
            st.error(f"âŒ í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompt_path}")
            return None
        except Exception as e:
            st.error(f"âŒ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

        prompt = ChatPromptTemplate.from_template(template)
        
        # 2) LLM ìƒì„± (temperature ê³ ì •: 0)
        if model.startswith("gpt"):
            llm = ChatOpenAI(
                model=model, 
                temperature=0,
                streaming=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            )
        elif model.startswith("claude"):
            from langchain_anthropic import ChatAnthropic
            llm = ChatAnthropic(
                model=model,
                temperature=0,
                streaming=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            )
        else:
            llm = ChatOpenAI(
                model="gpt-3.5-turbo", 
                temperature=0,
                streaming=True
            )
        
        # 3) ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜
        def format_docs(docs):
            if not docs:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            formatted_docs = []
            for i, doc in enumerate(docs, 1):
                source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                source_type = doc.metadata.get('source_type', 'ì•Œ ìˆ˜ ì—†ìŒ')
                doc_info = f"[ë¬¸ì„œ {i}] ({source}, {source_type})\n{doc.page_content}\n"
                formatted_docs.append(doc_info)
            
            return "\n".join(formatted_docs)
        
        # 4) Memory ìƒì„± (í† í° ì œí•œìœ¼ë¡œ ìë™ ìš”ì•½)
        if "conversation_memory" not in st.session_state:
            st.session_state.conversation_memory = ConversationSummaryBufferMemory(
                llm=llm,  # ìš”ì•½ì„ ìœ„í•œ LLM
                max_token_limit=1000,  # 1000 í† í° ì´ˆê³¼ ì‹œ ìš”ì•½
                memory_key="chat_history",
                return_messages=True
            )
        
        memory = st.session_state.conversation_memory
        
        # 5) Chain ìƒì„± (ë©”ëª¨ë¦¬ + ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
        def run_chain_with_memory(question):
            """ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•œ ì²´ì¸ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)"""
            
            # í˜„ì¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
            chat_history_data = memory.load_memory_variables({})
            chat_history = chat_history_data.get("chat_history", "ì´ì „ ëŒ€í™” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            if isinstance(chat_history, list):
                formatted_history = []
                for msg in chat_history:
                    if hasattr(msg, 'content'):
                        role = "ì‚¬ìš©ì" if msg.__class__.__name__ == "HumanMessage" else "AI"
                        formatted_history.append(f"{role}: {msg.content}")
                chat_history = "\n".join(formatted_history) or "ì´ì „ ëŒ€í™” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
            
            # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            docs = retriever.get_relevant_documents(question)
            context = format_docs(docs)
            
            # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… (chat_history í¬í•¨)
            formatted_prompt = prompt.format(
                chat_history=chat_history,
                context=context,
                question=question
            )
            
            return llm, formatted_prompt, memory, question
        
        return run_chain_with_memory
        
    except Exception as e:
        st.error(f"âŒ ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. ë©”ì¸ ë¡œì§: ì‹œìŠ¤í…œ ì´ˆê¸°í™” & QA ìˆ˜í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒë§Œ)
if not st.session_state["retriever_ready"]:
    with st.spinner("ğŸš€ ì‹œìŠ¤í…œ ì¤€ë¹„ ì¤‘..."):
        retriever = create_retriever()
        if retriever:
            st.session_state["chain"] = create_chain(retriever, selected_model)
            st.session_state["retriever_ready"] = True
        else:
            st.error("âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            st.stop()

# ì„¤ì • ë³€ê²½ ì‹œ ì²´ì¸ ì¬ìƒì„±
if st.session_state["retriever_ready"]:
    # ìƒˆë¡œìš´ ì²´ì¸ ìƒì„± (retrieverëŠ” ì¬ì‚¬ìš©)
    if st.session_state["chain"] is None:
        retriever = create_retriever()  # ìºì‹œëœ ê²ƒì´ ì‚¬ìš©ë¨
        st.session_state["chain"] = create_chain(retriever, selected_model)

# ì´ˆê¸°í™” ë²„íŠ¼ í´ë¦­ ì‹œ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì™€ ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ì²´ì¸ì€ ìœ ì§€)
if clear_btn:
    st.session_state["messages"] = []
    # ConversationMemory ì´ˆê¸°í™”
    if "conversation_memory" in st.session_state:
        st.session_state.conversation_memory.clear()
    st.rerun()

# ì±„íŒ… ì˜ì—­ (ì»¨í…Œì´ë„ˆë¡œ ê°ì‹¸ê¸°)
with st.container():
    # ì›°ì»´ ë©”ì‹œì§€ (ëŒ€í™” ê¸°ë¡ì´ ì—†ì„ ë•Œë§Œ í‘œì‹œ)
    if not st.session_state["messages"] and st.session_state["retriever_ready"]:
        st.markdown("""
            <div style="text-align: center; padding: 3rem 1rem; color: #6b7280; background: white; border-radius: 12px; margin-bottom: 2rem; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                <h3 style="color: #374151; margin-bottom: 1rem;">ğŸ‘‹ ì•ˆë…•í•˜ì„¸ìš”!</h3>
                <p style="margin-bottom: 2rem;">ë…¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì„ ììœ ë¡­ê²Œ ë¬¼ì–´ë³´ì„¸ìš”</p>
                
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem; margin-top: 2rem;">
                    <div style="background: #f9fafb; padding: 1rem; border-radius: 8px; border-left: 4px solid #3b82f6;">
                        <strong>ê¶Œê³ ì‚¬ì§ê³¼ í•´ê³ ì˜ ì°¨ì´</strong>
                    </div>
                    <div style="background: #f9fafb; padding: 1rem; border-radius: 8px; border-left: 4px solid #10b981;">
                        <strong>ì‹¤ì—…ê¸‰ì—¬ ì‹ ì²­ ë°©ë²•</strong>
                    </div>
                    <div style="background: #f9fafb; padding: 1rem; border-radius: 8px; border-left: 4px solid #f59e0b;">
                        <strong>ê·¼ë¡œê³„ì•½ì„œ ì‘ì„± ë°©ë²•</strong>
                    </div>
                    <div style="background: #f9fafb; padding: 1rem; border-radius: 8px; border-left: 4px solid #ef4444;">
                        <strong>ë¶€ë‹¹í•´ê³  ëŒ€ì‘ ë°©ë²•</strong>
                    </div>
                </div>
            </div>
        """, unsafe_allow_html=True)
    
    # í˜ì´ì§€ ë Œë”ë§ë§ˆë‹¤ ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
    print_messages()

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
user_input = st.chat_input("ë…¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (Shift + Enterë¡œ ì¤„ë°”ê¿ˆ)")
warning = st.empty()  # ë¹ˆ placeholder: ê²½ê³  ë©”ì‹œì§€ìš© 

if user_input:
    chain_func = st.session_state.get("chain")  # ì €ì¥ëœ chain í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    if chain_func is None:
        # chainì´ ì—†ìœ¼ë©´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì•ˆë‚´
        warning.error("ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•´ì£¼ì„¸ìš”.")
    else:
        # 1) ì‚¬ìš©ì ë©”ì‹œì§€ í™”ë©´ì— ì“°ê¸°
        st.chat_message("user").write(user_input)
        
        # 2) ë©”ëª¨ë¦¬ì™€ í•¨ê»˜ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        try:
            with st.chat_message("assistant"):
                container = st.empty()
                ai_answer = ""
                
                # Chain í•¨ìˆ˜ ì‹¤í–‰í•˜ì—¬ LLM, í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°
                llm, formatted_prompt, memory, question = chain_func(user_input)
                
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                for token in llm.stream(formatted_prompt):
                    ai_answer += token.content
                    container.markdown(ai_answer + "â–Œ")
                
                # ìµœì¢… ì‘ë‹µ í‘œì‹œ
                container.markdown(ai_answer)
                
                # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥ (í† í° ì´ˆê³¼ ì‹œ ìë™ ìš”ì•½)
                memory.save_context(
                    {"human": question},
                    {"ai": ai_answer}
                )
                
        except Exception as e:
            st.error(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            ai_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
        # 3) ëŒ€í™”ê¸°ë¡ìœ¼ë¡œ ì €ì¥ -> ë‹¤ìŒ ë Œë”ë§ ë•Œ ì¶œë ¥
        add_message("user", user_input)
        add_message("assistant", ai_answer) 