import streamlit as st
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

# LCEL ê´€ë ¨ ì¶”ê°€ ì„í¬íŠ¸
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableMap
from langchain_core.output_parsers import StrOutputParser

# ConversationSummaryBufferMemory ì‚¬ìš©
from langchain.memory import ConversationSummaryBufferMemory

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
import logging
from dotenv import load_dotenv
import os
import pickle
import yaml

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Streamlit í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•¨)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="HR RAG ì±„íŒ…ë´‡",
    page_icon="ğŸ¢",
    layout="wide",
    initial_sidebar_state="collapsed",
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. í™˜ê²½ ë³€ìˆ˜ ë° ë¡œê¹… ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
logging.langsmith("[Project] HR RAG ì±„íŒ…ë´‡")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ìºì‹œ ë””ë ‰í† ë¦¬ ì¤€ë¹„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for sub in [".cache", ".cache/files", ".cache/embeddings"]:
    if not os.path.exists(sub):
        os.mkdir(sub)

st.title("ğŸ¢ HR RAG ì±„íŒ…ë´‡")

# ì»¤ìŠ¤í…€ CSS ìŠ¤íƒ€ì¼
st.markdown(
    """
    <style>
    /* ì „ì²´ í˜ì´ì§€ ë°°ê²½ìƒ‰ ë° í°íŠ¸ ì„¤ì • */
    body {
        background-color: #1a1a1a;
        color: #e0e0e0;
        font-family: 'AppleSDGothicNeo-Regular', 'Noto Sans KR', sans-serif;
    }

    /* ì‚¬ì´ë“œë°” ìŠ¤íƒ€ì¼ */
    .st-emotion-cache-jx6q2s {
        background-color: #2b2b2b;
        color: #f0f0f0;
        padding: 20px;
        border-radius: 10px;
    }

    /* ì±„íŒ… ë©”ì‹œì§€ ë²„ë¸” ìŠ¤íƒ€ì¼ */
    .st-chat-message-container {
        border-radius: 15px;
        padding: 10px 15px;
        margin-bottom: 10px; 
    } 
    
    .st-chat-message-container.st-chat-message-assistant {
        background-color: #333333;
        margin-right: auto;
        border-bottom-left-radius: 5px;
        animation: fadeInUp 0.5s ease-out;
    }
    .st-chat-message-container.st-chat-message-user {
        background-color: #004d40;
        margin-left: auto;
        border-bottom-right-radius: 5px;
        animation: fadeInUp 0.3s ease-out;
    }
    
    /* ì›°ì»´ë©”ì‹œì§€ ë¶€ë“œëŸ¬ìš´ í˜ì´ë“œ íš¨ê³¼ */
    @keyframes fadeInUp {
        from {
            opacity: 0;
            transform: translateY(20px);
        }
        to {
            opacity: 1;
            transform: translateY(0);
        }
    }
    
    @keyframes fadeOut {
        from {
            opacity: 1;
            transform: translateY(0);
        }
        to {
            opacity: 0;
            transform: translateY(-10px);
        }
    }
    
    /* ì…ë ¥ì°½ ìŠ¤íƒ€ì¼ */
    .st-emotion-cache-x1bvup {
        background-color: #2b2b2b;
        border-radius: 15px;
        padding: 10px;
        border: none;
    }
    .st-emotion-cache-1c7y2qn textarea {
        background-color: #2b2b2b;
        color: #e0e0e0;
        border: none;
    }
    .st-emotion-cache-sey4o0 {
        background-color: #007bff;
        border-radius: 8px;
        color: white;
    }

    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton > button {
        background-color: #007bff;
        color: white;
        border-radius: 8px;
        border: none;
        padding: 10px 20px;
        font-weight: bold;
        transition: all 0.3s ease;
    }
    .stButton > button:hover {
        background-color: #0056b3;
        transform: translateY(-1px);
    }
    
    /* ì¤‘ì§€ ë²„íŠ¼ íŠ¹ë³„ ìŠ¤íƒ€ì¼ */
    button[title="ë‹µë³€ ì¤‘ì§€"] {
        background-color: #dc3545 !important;
        color: white !important;
        font-size: 16px !important;
        font-weight: bold !important;
        border-radius: 8px !important;
        transition: all 0.2s ease !important;
    }
    
    button[title="ë‹µë³€ ì¤‘ì§€"]:hover {
        background-color: #c82333 !important;
        transform: scale(1.05) !important;
    }
    
    /* ë¹„í™œì„±í™”ëœ ì…ë ¥ì°½ ìŠ¤íƒ€ì¼ */
    .stTextInput input:disabled {
        background-color: #3a3a3a !important;
        color: #888 !important;
        border: 1px solid #555 !important;
    }
    
    /* ì •ë³´ ë°•ìŠ¤ ìŠ¤íƒ€ì¼ */
    .st-emotion-cache-1r6dm7m {
        background-color: #3a3a3a;
        color: #b3e0ff;
        border-left: 5px solid #007bff;
        border-radius: 8px;
        padding: 15px;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "chain" not in st.session_state:
    st.session_state["chain"] = None

if "retriever_ready" not in st.session_state:
    st.session_state["retriever_ready"] = False

# ë‹µë³€ ì¤‘ì§€ í”Œë˜ê·¸ ì´ˆê¸°í™”
if "stop_generation" not in st.session_state:
    st.session_state.stop_generation = False

# LLM ìƒì„± ì¤‘ì¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í”Œë˜ê·¸ (UI ì œì–´ìš©)
if "is_generating" not in st.session_state:
    st.session_state.is_generating = False

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ì¤‘ì¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í”Œë˜ê·¸ (ì¤‘ë³µ ë°©ì§€ìš©)
if "processing_user_input" not in st.session_state:
    st.session_state.processing_user_input = False

# ì‚¬ìš©ì ì…ë ¥ì´ í•œ ë²ˆì´ë¼ë„ ìˆì—ˆëŠ”ì§€ ì¶”ì í•˜ëŠ” í”Œë˜ê·¸ (ì›°ì»´ ë©”ì‹œì§€ ì œì–´ìš©)
if "user_input_detected" not in st.session_state:
    st.session_state.user_input_detected = False

# ConversationSummaryBufferMemory ì‹œìŠ¤í…œ ì´ˆê¸°í™”
if "memory_store" not in st.session_state:
    st.session_state["memory_store"] = {}

# ë©€í‹° ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬
if "chat_sessions" not in st.session_state:
    st.session_state["chat_sessions"] = {}  # {session_id: {"title": str, "created_at": datetime}}
    
if "current_session_id" not in st.session_state:
    # ì²« ì„¸ì…˜ ìë™ ìƒì„±
    import uuid
    from datetime import datetime
    first_session_id = str(uuid.uuid4())[:8]
    st.session_state["current_session_id"] = first_session_id
    st.session_state["chat_sessions"][first_session_id] = {
        "title": "ìƒˆë¡œìš´ ëŒ€í™”",
        "created_at": datetime.now()
    }

if "session_counter" not in st.session_state:
    st.session_state["session_counter"] = 1

# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ConversationSummaryBufferMemoryë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_memory(session_id: str):
    """ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ConversationSummaryBufferMemoryë¥¼ ë°˜í™˜"""
    if session_id not in st.session_state["memory_store"]:
        # ì„ì‹œ LLMìœ¼ë¡œ ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ë‚˜ì¤‘ì— ì‹¤ì œ LLMìœ¼ë¡œ êµì²´)
        temp_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        st.session_state["memory_store"][session_id] = ConversationSummaryBufferMemory(
            llm=temp_llm,
            max_token_limit=1000,
            return_messages=True,
            memory_key="chat_history",
        )
    return st.session_state["memory_store"][session_id]

# ë©€í‹° ì„¸ì…˜ ê´€ë¦¬ í•¨ìˆ˜ë“¤
def create_new_session():
    """ìƒˆë¡œìš´ ëŒ€í™” ì„¸ì…˜ ìƒì„±"""
    import uuid
    from datetime import datetime
    
    new_session_id = str(uuid.uuid4())[:8]
    st.session_state["chat_sessions"][new_session_id] = {
        "title": "ìƒˆë¡œìš´ ëŒ€í™”",
        "created_at": datetime.now()
    }
    st.session_state["current_session_id"] = new_session_id
    st.session_state["messages"] = []  # ìƒˆ ì„¸ì…˜ì˜ UI ë©”ì‹œì§€ ì´ˆê¸°í™”
    st.session_state.user_input_detected = False  # ì›°ì»´ ë©”ì‹œì§€ í‘œì‹œë¥¼ ìœ„í•´
    return new_session_id

def delete_session(session_id: str):
    """ëŒ€í™” ì„¸ì…˜ ì‚­ì œ"""
    if session_id in st.session_state["chat_sessions"]:
        del st.session_state["chat_sessions"][session_id]
    
    if session_id in st.session_state["memory_store"]:
        del st.session_state["memory_store"][session_id]
    
    # í˜„ì¬ ì„¸ì…˜ì´ ì‚­ì œëœ ê²½ìš° ë‹¤ë¥¸ ì„¸ì…˜ìœ¼ë¡œ ì „í™˜
    if st.session_state["current_session_id"] == session_id:
        remaining_sessions = list(st.session_state["chat_sessions"].keys())
        if remaining_sessions:
            switch_to_session(remaining_sessions[0])
        else:
            # ëª¨ë“  ì„¸ì…˜ì´ ì‚­ì œëœ ê²½ìš° ìƒˆ ì„¸ì…˜ ìƒì„±
            create_new_session()

def switch_to_session(session_id: str):
    """ë‹¤ë¥¸ ì„¸ì…˜ìœ¼ë¡œ ì „í™˜"""
    st.session_state["current_session_id"] = session_id
    
    # í•´ë‹¹ ì„¸ì…˜ì˜ UI ë©”ì‹œì§€ ë¡œë“œ (ë©”ëª¨ë¦¬ì—ì„œ ë³µì›)
    if session_id in st.session_state["memory_store"]:
        memory = st.session_state["memory_store"][session_id]
        messages = memory.chat_memory.messages
        
        # UI ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ì¬êµ¬ì„±
        st.session_state["messages"] = []
        for msg in messages:
            role = "user" if msg.type == "human" else "assistant"
            st.session_state["messages"].append(ChatMessage(role=role, content=msg.content))
    else:
        st.session_state["messages"] = []
    
    # ì›°ì»´ ë©”ì‹œì§€ í‘œì‹œ ì—¬ë¶€ ê²°ì •
    st.session_state.user_input_detected = len(st.session_state["messages"]) > 0

def generate_title_from_question(question: str):
    """ì²« ë²ˆì§¸ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ëŒ€í™” ì œëª© ìƒì„±"""
    # ê°„ë‹¨í•œ ì œëª© ìƒì„± ë¡œì§
    if len(question) > 30:
        return question[:27] + "..."
    return question

def update_session_title(session_id: str, new_title: str):
    """ì„¸ì…˜ ì œëª© ì—…ë°ì´íŠ¸"""
    if session_id in st.session_state["chat_sessions"]:
        st.session_state["chat_sessions"][session_id]["title"] = new_title


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ì‚¬ì´ë“œë°” êµ¬ì„± (ChatGPT ìŠ¤íƒ€ì¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    # ìƒˆ ëŒ€í™” ë²„íŠ¼
    if st.button("â• ìƒˆ ëŒ€í™”", use_container_width=True, type="primary"):
        create_new_session()
        st.rerun()
    
    st.divider()
    
    # ëŒ€í™” ëª©ë¡
    st.subheader("ğŸ’¬ ëŒ€í™” ëª©ë¡")
    
    current_session = st.session_state["current_session_id"]
    
    # ëŒ€í™” ì„¸ì…˜ë“¤ì„ ìƒì„± ì‹œê°„ ì—­ìˆœìœ¼ë¡œ ì •ë ¬
    sessions = st.session_state["chat_sessions"]
    sorted_sessions = sorted(sessions.items(), 
                           key=lambda x: x[1]["created_at"], 
                           reverse=True)
    
    for session_id, session_data in sorted_sessions:
        title = session_data["title"]
        is_current = session_id == current_session
        
        # í˜„ì¬ ì„ íƒëœ ì„¸ì…˜ì€ ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ë¡œ í‘œì‹œ
        if is_current:
            st.markdown(f"**ğŸŸ¢ {title}**")
        else:
            # ëŒ€í™” ì„ íƒ ë²„íŠ¼ê³¼ ì‚­ì œ ë²„íŠ¼ì„ í•œ ì¤„ì—
            col1, col2 = st.columns([0.8, 0.2])
            
            with col1:
                if st.button(title, key=f"select_{session_id}", use_container_width=True):
                    switch_to_session(session_id)
                    st.rerun()
            
            with col2:
                if st.button("ğŸ—‘ï¸", key=f"delete_{session_id}", help="ëŒ€í™” ì‚­ì œ"):
                    delete_session(session_id)
                    st.rerun()
    
    # ëŒ€í™”ê°€ ì—†ëŠ” ê²½ìš°
    if not sessions:
        st.info("ì•„ì§ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.divider()
    
    # ì„¤ì • ì„¹ì…˜
    st.subheader("âš™ï¸ ì„¤ì •")

    # ëª¨ë¸ ì„ íƒ
    selected_model = st.selectbox(
        "ğŸ¤– LLM ëª¨ë¸",
        ["gpt-3.5-turbo", "gpt-4o-mini", "gpt-4o"],
        index=1,
    )

    # ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜
    k = st.number_input("ğŸ“„ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜", min_value=1, max_value=10, value=3)
    
    st.divider()

    # í˜„ì¬ ì„¸ì…˜ ë©”ëª¨ë¦¬ ìƒíƒœ í‘œì‹œ
    if current_session in st.session_state.get("memory_store", {}):
        session_memory = st.session_state["memory_store"][current_session]
        # ConversationSummaryBufferMemoryì˜ ë©”ì‹œì§€ëŠ” chat_memory.messagesì— ìˆìŒ
        message_count = len(session_memory.chat_memory.messages)
        turns = message_count // 2
        st.info(f"ğŸ’­ í˜„ì¬ ëŒ€í™”: {turns}í„´ ì €ì¥ì¤‘")
    else:
        st.info(f"ğŸ’­ í˜„ì¬ ëŒ€í™”: ìƒˆë¡œìš´ ëŒ€í™”")

    # í˜„ì¬ ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ—‘ï¸ í˜„ì¬ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
        if current_session in st.session_state.get("memory_store", {}):
            st.session_state["memory_store"][current_session].clear()
        st.session_state["messages"] = []
        st.session_state.user_input_detected = False
        st.rerun()

    st.divider()

    # ì‹œìŠ¤í…œ ì •ë³´
    st.info(
        """
    **ğŸ¢ HR RAG ì±„íŒ…ë´‡**
    
    ë…¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì— ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    
    **íŠ¹ì§•:**
    - ğŸ§  ì´ì „ ëŒ€í™” ê¸°ì–µ
    - ğŸ“š 143ê°œ ì „ë¬¸ ë¬¸ì„œ ê¸°ë°˜
    - âš–ï¸ ë²•ì  ê·¼ê±° ì œì‹œ
    - ğŸ”„ ë©€í‹°í„´ ëŒ€í™” ì§€ì›
    - ğŸ’¬ ë‹¤ì¤‘ ëŒ€í™” ì„¸ì…˜
    """
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. í—¬í¼ í•¨ìˆ˜ë“¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def print_messages():
    """ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ì„ í™”ë©´ì— ì¶œë ¥"""
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


def add_message(role: str, content: str):
    """ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ì— ì¶”ê°€"""
    st.session_state["messages"].append(ChatMessage(role=role, content=content))


def format_docs(docs):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…í•  í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…"""
    return "\n\n".join(doc.page_content for doc in docs)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. HR Documents â†’ FAISS Retriever ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def create_retriever():
    """ê¸°ì¡´ì— ì „ì²˜ë¦¬ëœ documents.pklì—ì„œ FAISS ê²€ìƒ‰ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""

    try:
        # ê°„ë‹¨í•œ ê²½ë¡œ í•´ê²°
        # 1. í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ê²½ë¡œ
        current_dir = os.path.dirname(os.path.abspath(__file__))
        documents_path = os.path.join(current_dir, 'data', 'processed', 'documents.pkl')
        
        # 2. ì•ˆ ë˜ë©´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€
        if not os.path.exists(documents_path):
            project_root = os.path.join(current_dir, '..', '..')  # src/preprocessingì—ì„œ 2ë‹¨ê³„ ìœ„ë¡œ
            documents_path = os.path.join(project_root, 'data', 'processed', 'documents.pkl')
        
        # 3. ê·¸ë˜ë„ ì•ˆ ë˜ë©´ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€
        if not os.path.exists(documents_path):
            documents_path = os.path.join(os.getcwd(), 'data', 'processed', 'documents.pkl')
        
        if not os.path.exists(documents_path):
            st.error(f"âŒ documents.pkl íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.info("ğŸ’¡ í•´ê²°ë°©ë²•: í„°ë¯¸ë„ì—ì„œ '000. Project_rag' í´ë”ë¡œ ì´ë™ í›„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return None

        with open(documents_path, 'rb') as f:
            documents = pickle.load(f)

        # 2) í…ìŠ¤íŠ¸ ë¶„í• 
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        split_docs = splitter.split_documents(documents)

        # 3) ì„ë² ë”© ìƒì„± (OpenAI ìš°ì„  ì‚¬ìš©)
        openai_api_key = os.getenv("OPENAI_API_KEY")

        if openai_api_key:
            embeddings = OpenAIEmbeddings()
        else:
            st.error("âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        # 4) FAISS ì¸ë±ìŠ¤ ìƒì„±
        vectorstore = FAISS.from_documents(documents=split_docs, embedding=embeddings)

        return vectorstore

    except Exception as e:
        st.error(f"âŒ ê²€ìƒ‰ê¸° ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# get_retriever í•¨ìˆ˜ì—ì„œ score_threshold ë§¤ê°œë³€ìˆ˜ ì œê±°
def get_retriever(vectorstore, k=3):
    """Vectorstoreì—ì„œ ê²€ìƒ‰ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return vectorstore.as_retriever(search_kwargs={"k": k})


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8. ìƒˆë¡œìš´ ì²´ì¸ ìƒì„± í•¨ìˆ˜ (ConversationSummaryBufferMemory ë°©ì‹)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def create_chain(retriever, model="gpt-3.5-turbo"):
    """ConversationSummaryBufferMemoryë¥¼ ì‚¬ìš©í•œ RAG ì²´ì¸ ìƒì„±"""
    
    try:
        # 1. í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        current_dir = os.path.dirname(os.path.abspath(__file__))
        prompt_path = os.path.join(current_dir, 'prompt', 'qa_prompt.yaml')
        
        if not os.path.exists(prompt_path):
            st.error(f"âŒ í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompt_path}")
            return None

        with open(prompt_path, 'r', encoding='utf-8') as f:
            prompt_config = yaml.safe_load(f)

        # í”„ë¡¬í”„íŠ¸ ì„¤ì • (chat_historyë¥¼ ë¬¸ìì—´ë¡œ ì²˜ë¦¬)
        system_prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ ë…¸ë¬´ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. 

**ì‘ë‹µ ì§€ì¹¨:**
1. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì—°ì†ì ì´ê³  ì¼ê´€ëœ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
2. ë²•ì  ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•´ì£¼ì„¸ìš”.
3. ì‹¤ë¬´ì ì¸ ì ˆì°¨ì™€ ë°©ë²•ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
4. ì£¼ì˜ì‚¬í•­ì´ë‚˜ ì˜ˆì™¸ ìƒí™©ë„ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”.

**ì´ì „ ëŒ€í™” ê¸°ë¡:**
{chat_history}

**ì°¸ê³  ë¬¸ì„œ:**
{context}"""

        # ChatPromptTemplate ìƒì„± (MessagesPlaceholder ì—†ì´ ë‹¨ìˆœí•œ ë¬¸ìì—´ ë°©ì‹)
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{question}")
        ])

        # 2. LLM ìƒì„±
        if model.startswith("gpt"):
            llm = ChatOpenAI(model=model, temperature=0, streaming=True)
        else:
            llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, streaming=True)

        # 3. ì²´ì¸ ìƒì„± í•¨ìˆ˜ (ë©”ëª¨ë¦¬ì™€ í•¨ê»˜ ì‹¤í–‰)
        def run_chain_with_memory(question: str, session_id: str):
            """ë©”ëª¨ë¦¬ë¥¼ í¬í•¨í•˜ì—¬ ì²´ì¸ ì‹¤í–‰"""
            # í˜„ì¬ ì„¸ì…˜ì˜ ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°
            memory = get_session_memory(session_id)
            
            # ë©”ëª¨ë¦¬ LLM ì—…ë°ì´íŠ¸ (í˜„ì¬ ì„ íƒëœ ëª¨ë¸ë¡œ)
            memory.llm = llm
            
            # ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
            memory_variables = memory.load_memory_variables({})
            chat_history = memory_variables.get("chat_history", "")
            
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            docs = retriever.get_relevant_documents(question)
            context = format_docs(docs)
            
            # í”„ë¡¬í”„íŠ¸ì— ë³€ìˆ˜ ì „ë‹¬
            formatted_prompt = prompt.format_messages(
                chat_history=chat_history,
                context=context,
                question=question
            )
            
            # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´ í•„ìš”í•œ ì •ë³´ ë°˜í™˜
            return {
                "llm": llm,
                "formatted_prompt": formatted_prompt,
                "memory": memory,
                "question": question
            }
        
        return run_chain_with_memory

    except Exception as e:
        st.error(f"âŒ ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(f"âŒ ì²´ì¸ ìƒì„± ì˜¤ë¥˜: {e}")
        print(f"âŒ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 9. ë©”ì¸ ë¡œì§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒë§Œ)
if not st.session_state["retriever_ready"]:
    with st.spinner("ğŸš€ ì‹œìŠ¤í…œ ì¤€ë¹„ ì¤‘..."):
        # ìºì‹œ í´ë¦¬ì–´ (ì„ë² ë”© ë³€ê²½ìœ¼ë¡œ ì¸í•´)
        st.cache_resource.clear()
        
        vectorstore = create_retriever()
        if vectorstore:
            retriever = get_retriever(vectorstore, k)
            st.session_state["vectorstore"] = vectorstore
            st.session_state["chain"] = create_chain(retriever, selected_model)
            st.session_state["retriever_ready"] = True
        else:
            st.error("âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            st.stop()

# ì„¤ì • ë³€ê²½ ì‹œ ì²´ì¸ ì¬ìƒì„±
# ì‚¬ì´ë“œë°” ì„¤ì • (k, selected_model) ë³€ê²½ ê°ì§€
# ë³€ê²½ì´ ê°ì§€ë˜ë©´ ì²´ì¸ì„ ì¬ìƒì„±í•©ë‹ˆë‹¤.
if st.session_state["retriever_ready"]:
    # ì´ì „ ì„¤ì •ê°’ë“¤ ì €ì¥ ë° ë¹„êµ
    prev_k = st.session_state.get("prev_k", None)
    prev_model = st.session_state.get("prev_model", None)
    
    # ì„¤ì •ì´ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ ì²´ì¸ ì¬ìƒì„±
    if prev_k != k or prev_model != selected_model:
        # ğŸ”§ ëª¨ë¸ ë³€ê²½ ì‹œ UI ìƒíƒœ í”Œë˜ê·¸ ì´ˆê¸°í™”
        st.session_state.is_generating = False
        st.session_state.stop_generation = False
        st.session_state.processing_user_input = False
        
        try:
            with st.spinner("ğŸ”„ ì„¤ì • ë³€ê²½ ì¤‘..."):
                current_retriever = get_retriever(st.session_state["vectorstore"], k)
                new_chain = create_chain(current_retriever, selected_model)
                
                if new_chain is not None:
                    st.session_state["chain"] = new_chain
                    # í˜„ì¬ ì„¤ì •ê°’ ì €ì¥
                    st.session_state["prev_k"] = k
                    st.session_state["prev_model"] = selected_model
                    
                    # ì„¤ì • ë³€ê²½ ì•Œë¦¼ (ì ê¹ í‘œì‹œ)
                    if prev_model is not None and prev_model != selected_model:
                        st.success(f"âœ… ëª¨ë¸ì´ {selected_model}ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ UI ìƒíƒœ ì™„ì „ ì´ˆê¸°í™”
                        st.rerun()
                else:
                    st.error("âŒ ìƒˆë¡œìš´ ì„¤ì •ìœ¼ë¡œ ì²´ì¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    # ì´ì „ ëª¨ë¸ë¡œ ë¡¤ë°± ì‹œë„
                    if prev_model and prev_model != selected_model:
                        st.info(f"ğŸ”„ ì´ì „ ëª¨ë¸({prev_model})ë¡œ ë¡¤ë°±ì„ ì‹œë„í•©ë‹ˆë‹¤...")
                        try:
                            rollback_chain = create_chain(current_retriever, prev_model)
                            if rollback_chain is not None:
                                st.session_state["chain"] = rollback_chain
                                st.warning(f"âš ï¸ {prev_model} ëª¨ë¸ë¡œ ë¡¤ë°±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            else:
                                st.error("âŒ ë¡¤ë°±ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•´ì£¼ì„¸ìš”.")
                        except Exception as rollback_error:
                            st.error(f"âŒ ë¡¤ë°± ì¤‘ ì˜¤ë¥˜: {str(rollback_error)}")
        except Exception as e:
            st.error(f"âŒ ì„¤ì • ë³€ê²½ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # íŠ¹ì • ì˜¤ë¥˜ ë©”ì‹œì§€ì— ë”°ë¥¸ ì•ˆë‚´
            if "duplicate validator" in str(e):
                st.info("ğŸ’¡ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶©ëŒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            
            # ì˜¤ë¥˜ ì‹œì—ë„ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.is_generating = False
            st.session_state.stop_generation = False
            st.session_state.processing_user_input = False
    else:
        # ì²« ì‹¤í–‰ ì‹œ ì„¤ì •ê°’ ì €ì¥
        if prev_k is None:
            st.session_state["prev_k"] = k
            st.session_state["prev_model"] = selected_model


# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
def print_previous_messages():
    """í˜„ì¬ ì„¸ì…˜ì˜ ì´ì „ ë©”ì‹œì§€ë“¤ì„ ì¶œë ¥"""
    messages_to_show = st.session_state["messages"]
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ì¤‘ì´ë©´ ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” ì œì™¸ (ì¤‘ë³µ ë°©ì§€)
    # ì™œëƒí•˜ë©´ ì´ ë©”ì‹œì§€ëŠ” ì‚¬ìš©ì ì…ë ¥ ì‹œì ì— ì´ë¯¸ ì§ì ‘ ì¶œë ¥ë˜ê¸° ë•Œë¬¸
    if st.session_state.get('processing_user_input', False):
        if len(messages_to_show) > 0 and messages_to_show[-1].role == "user":
            messages_to_show = messages_to_show[:-1]
    
    for chat_message in messages_to_show:
        st.chat_message(chat_message.role).write(chat_message.content)

# ì´ì „ ëŒ€í™” ê¸°ë¡ í‘œì‹œ
print_previous_messages()

# ì›°ì»´ ë©”ì‹œì§€ í‘œì‹œ ì—¬ë¶€ ê²°ì • (ë” ì¦‰ê°ì ì¸ ë°˜ì‘ì„ ìœ„í•´)
show_welcome = (len(st.session_state["messages"]) == 0 and 
                st.session_state.get("retriever_ready", False) and 
                not st.session_state.get("user_input_detected", False))

# ì²« ë°©ë¬¸ ì‹œ ì›°ì»´ ë©”ì‹œì§€
if show_welcome:
    with st.chat_message("assistant"):
        st.markdown(
            """
            <div style="animation: fadeInUp 0.8s ease-out;">
            
            ì•ˆë…•í•˜ì„¸ìš”! ğŸ¢ **HR RAG ì±„íŒ…ë´‡**ì…ë‹ˆë‹¤.
            
            ë…¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì„ ììœ ë¡­ê²Œ ë¬¼ì–´ë³´ì„¸ìš”:
            - ê¶Œê³ ì‚¬ì§ê³¼ í•´ê³ ì˜ ì°¨ì´
            - ì‹¤ì—…ê¸‰ì—¬ ì‹ ì²­ ë°©ë²•  
            - ê·¼ë¡œê³„ì•½ì„œ ì‘ì„± ë°©ë²•
            - ê¸°íƒ€ ë…¸ë¬´ ê´€ë ¨ ë²•ë¥  ë¬¸ì˜
            
            ì–¸ì œë“ ì§€ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ’¬
            
            </div>
            """,
            unsafe_allow_html=True
        )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë™ì  ì‚¬ìš©ì ì…ë ¥ (Send â†” Stop ë²„íŠ¼ ì „í™˜)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
user_input = None

if st.session_state.get('is_generating', False):
    # LLM ë‹µë³€ ì¤‘: ì…ë ¥ì°½ì„ ë¹„í™œì„±í™”í•˜ê³  ì¤‘ì§€ ë²„íŠ¼ìœ¼ë¡œ ë³€ê²½
    col1, col2 = st.columns([0.85, 0.15])
    
    with col1:
        st.text_input(
            "ë…¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...", 
            value="ë‹µë³€ ìƒì„± ì¤‘...", 
            disabled=True,
            key="disabled_input",
            label_visibility="collapsed"
        )
    
    with col2:
        if st.button("â¹ï¸", key="stop_button", help="ë‹µë³€ ì¤‘ì§€", use_container_width=True, type="primary"):
            st.session_state.stop_generation = True
            st.session_state.is_generating = False
            st.session_state.processing_user_input = False
            st.info("ë‹µë³€ ìƒì„±ì„ ì¤‘ì§€í–ˆìŠµë‹ˆë‹¤.")
            st.rerun()
            
    # ğŸ†˜ ì¶”ê°€ ì•ˆì „ì¥ì¹˜: ê°•ì œ ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ”„ ìƒíƒœ ì´ˆê¸°í™”", key="force_reset", help="UI ìƒíƒœë¥¼ ê°•ì œë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤"):
        st.session_state.is_generating = False
        st.session_state.stop_generation = False
        st.session_state.processing_user_input = False
        st.success("âœ… ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.rerun()
else:
    # ì¼ë°˜ ìƒíƒœ: í‘œì¤€ chat_input ì‚¬ìš©
    user_input = st.chat_input("ë…¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...", key="chat_input")

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_input:
    # ğŸ”¥ ì›°ì»´ë©”ì‹œì§€ ì¦‰ì‹œ ìˆ¨ê¸°ê¸° (ê°€ì¥ ë¨¼ì € ì‹¤í–‰)
    st.session_state.user_input_detected = True
    
    # í”Œë˜ê·¸ ì„¤ì •
    st.session_state.stop_generation = False
    st.session_state.is_generating = True
    st.session_state.processing_user_input = True

    # ì²´ì¸ ê°€ì ¸ì˜¤ê¸°
    chain = st.session_state.get("chain")
    if chain is None:
        st.error("âŒ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.info("ğŸ’¡ ëª¨ë¸ì„ ë‹¤ì‹œ ì„ íƒí•˜ê±°ë‚˜ í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•´ì£¼ì„¸ìš”.")
        
        # ìë™ ì²´ì¸ ì¬ìƒì„± ì‹œë„
        if st.session_state.get("retriever_ready", False):
            try:
                with st.spinner("ğŸ”„ ì‹œìŠ¤í…œ ì¬ì´ˆê¸°í™” ì¤‘..."):
                    current_retriever = get_retriever(st.session_state["vectorstore"], k)
                    new_chain = create_chain(current_retriever, selected_model)
                    
                    if new_chain is not None:
                        st.session_state["chain"] = new_chain
                        st.success("âœ… ì‹œìŠ¤í…œì´ ì¬ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!")
                        st.rerun()
                    else:
                        st.error("âŒ ì‹œìŠ¤í…œ ì¬ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"âŒ ì¬ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        st.stop()

    # ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì •ëœ current_session_id ì‚¬ìš©
    current_session_id = st.session_state["current_session_id"]

    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° ì¦‰ì‹œ í™”ë©´ì— í‘œì‹œ
    add_message("user", user_input)
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì¦‰ì‹œ í™”ë©´ì— í‘œì‹œ (AI ì‘ë‹µ ê¸°ë‹¤ë¦¬ì§€ ì•ŠìŒ)
    with st.chat_message("user"):
        st.write(user_input)

    # 2. AI ì‘ë‹µ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ
    ai_answer = ""
    with st.chat_message("assistant"):
        container = st.empty()
        generation_stopped = False # ì‹¤ì œë¡œ ì¤‘ë‹¨ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ì¶”ì 

        try:
            # ConversationSummaryBufferMemory ì²´ì¸ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°)
            # ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì •ëœ current_session_id ì‚¬ìš©
            current_session_id = st.session_state["current_session_id"]

            # ì²´ì¸ í•¨ìˆ˜ í˜¸ì¶œí•˜ì—¬ í•„ìš”í•œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            chain_info = chain(user_input, current_session_id)
            llm = chain_info["llm"]
            formatted_prompt = chain_info["formatted_prompt"]
            memory = chain_info["memory"]
            question = chain_info["question"]
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            for chunk in llm.stream(formatted_prompt):
                if st.session_state.stop_generation:
                    generation_stopped = True
                    break # ì¤‘ì§€ í”Œë˜ê·¸ê°€ Trueì´ë©´ ë£¨í”„ë¥¼ ì¦‰ì‹œ ì¤‘ë‹¨
                
                ai_answer += chunk.content
                
                # ìŠ¤íŠ¸ë¦¬ë° UI ì—…ë°ì´íŠ¸
                container.markdown(ai_answer + "â–Œ") # í˜„ì¬ê¹Œì§€ì˜ ë‹µë³€ + ì»¤ì„œ í‘œì‹œ

            # ìµœì¢… ì‘ë‹µ í‘œì‹œ ë˜ëŠ” ì¤‘ì§€ ë©”ì‹œì§€
            if generation_stopped:
                container.markdown(ai_answer + "\n\n**ë‹µë³€ ìƒì„±ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.**")
            else:
                container.markdown(ai_answer)  # ìµœì¢… ë‹µë³€ (ì»¤ì„œ ì œê±°)

            # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥ (ì™„ì „í•œ ë‹µë³€ë§Œ ì €ì¥)
            if not generation_stopped and ai_answer:
                memory.save_context(
                    inputs={"human": question},
                    outputs={"ai": ai_answer}
                )
                
                # ì²« ë²ˆì§¸ ì§ˆë¬¸ì¸ ê²½ìš° ëŒ€í™” ì œëª© ìë™ ìƒì„±
                if len(memory.chat_memory.messages) == 2:  # human + ai = 2ê°œ (ì²« ë²ˆì§¸ ëŒ€í™”)
                    current_title = st.session_state["chat_sessions"][current_session_id]["title"]
                    if current_title == "ìƒˆë¡œìš´ ëŒ€í™”":  # ê¸°ë³¸ ì œëª©ì¸ ê²½ìš°ë§Œ ì—…ë°ì´íŠ¸
                        new_title = generate_title_from_question(question)
                        update_session_title(current_session_id, new_title)
                
                add_message("assistant", ai_answer)

        except Exception as e:
            st.error(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            ai_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            container.markdown(ai_answer)
            generation_stopped = True # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì €ì¥í•˜ì§€ ì•Šë„ë¡ ì²˜ë¦¬

        finally:
            # LLM ìƒì„± ì™„ë£Œ ë˜ëŠ” ì¤‘ë‹¨ ì‹œ is_generating í”Œë˜ê·¸ë¥¼ Falseë¡œ ì„¤ì •
            st.session_state.is_generating = False
            st.session_state.processing_user_input = False # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ì¤‘ í”Œë˜ê·¸ í•´ì œ
            st.rerun() # UIë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ì¤‘ì§€ ë²„íŠ¼ ìˆ¨ê¸°ê¸°