#!/usr/bin/env python3
"""
ğŸ¢ HR RAG ì±„íŒ… ì•±

ê¸°ì¡´ RAG ì‹œìŠ¤í…œì„ í™œìš©í•œ Streamlit ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜
- ChatGPT ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤
- ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ì¶œë ¥
- ì±„íŒ… ê¸°ë¡ ìœ ì§€
- ì‚¬ì´ë“œë°” ì„¤ì • ì˜µì…˜
- ì™„ì „í•œ ì—ëŸ¬ ì²˜ë¦¬
"""

import streamlit as st
import sys
import os
import time
from typing import Generator, Optional

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¢ HR RAG ì±„íŒ…ë´‡",
    page_icon="ğŸ¢",
    layout="wide",
    initial_sidebar_state="expanded"
)

def init_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "retriever" not in st.session_state:
        st.session_state.retriever = None
    if "qa_chain" not in st.session_state:
        st.session_state.qa_chain = None
    if "system_ready" not in st.session_state:
        st.session_state.system_ready = False

def load_rag_system(temperature: float = 0.1, k: int = 3):
    """RAG ì‹œìŠ¤í…œ ë¡œë“œ ë° ì´ˆê¸°í™”"""
    try:
        # ìºì‹±ì„ ìœ„í•´ ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ì¬ì‚¬ìš©
        if st.session_state.system_ready and st.session_state.retriever is not None:
            # temperature ë³€ê²½ ì‹œì—ë§Œ QA ì²´ì¸ ì¬ìƒì„±
            if hasattr(st.session_state, 'last_temperature') and st.session_state.last_temperature != temperature:
                st.session_state.qa_chain = create_qa_chain_with_temperature(st.session_state.retriever, temperature, k)
                st.session_state.last_temperature = temperature
            return True
        
        # ì²˜ìŒ ë¡œë“œí•˜ëŠ” ê²½ìš°
        with st.spinner("ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."):
            # retriever ë¡œë“œ
            from src.preprocessing.retriever import initialize_retriever
            
            processed_data_path = os.path.join(current_dir, 'data', 'processed', 'documents.pkl')
            retriever = initialize_retriever(processed_data_path)
            
            if not retriever:
                st.error("âŒ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False
            
            st.session_state.retriever = retriever
            
            # QA ì²´ì¸ ìƒì„±
            qa_chain = create_qa_chain_with_temperature(retriever, temperature, k)
            if not qa_chain:
                st.error("âŒ QA ì²´ì¸ ìƒì„± ì‹¤íŒ¨")
                return False
                
            st.session_state.qa_chain = qa_chain
            st.session_state.system_ready = True
            st.session_state.last_temperature = temperature
            
            st.success("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
            return True
            
    except Exception as e:
        st.error(f"âŒ RAG ì‹œìŠ¤í…œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def create_qa_chain_with_temperature(retriever, temperature: float, k: int):
    """ì§€ì •ëœ temperatureë¡œ QA ì²´ì¸ ìƒì„±"""
    try:
        from langchain_openai import ChatOpenAI
        from langchain_upstage import ChatUpstage
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        from langchain_core.runnables import RunnablePassthrough
        
        # LLM ì´ˆê¸°í™”
        openai_api_key = os.getenv("OPENAI_API_KEY")
        upstage_api_key = os.getenv("UPSTAGE_API_KEY")
        
        if openai_api_key:
            llm = ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=temperature,
                max_tokens=1000,
                streaming=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            )
        elif upstage_api_key:
            llm = ChatUpstage(
                model="solar-1-mini-chat",
                temperature=temperature,
                max_tokens=1000,
                streaming=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            )
        else:
            st.error("âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        template = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ë…¸ë¬´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

**ë‹µë³€ ì›ì¹™:**
1. ë²•ì  ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•´ì£¼ì„¸ìš”
2. ì‹¤ë¬´ì ì¸ ì ˆì°¨ì™€ ë°©ë²•ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´í•´ì£¼ì„¸ìš”  
3. ì£¼ì˜ì‚¬í•­ì´ë‚˜ ì˜ˆì™¸ ìƒí™©ë„ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”
4. ê´€ë ¨ ê¸°ê´€ì´ë‚˜ ë‹´ë‹¹ ë¶€ì„œ ì •ë³´ë„ í¬í•¨í•´ì£¼ì„¸ìš”

**ì°¸ê³  ë¬¸ì„œ:**
{context}

**ì§ˆë¬¸:** {question}

**ë‹µë³€:**"""

        prompt = ChatPromptTemplate.from_template(template)
        
        # ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜
        def format_docs(docs):
            if not docs:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            formatted_docs = []
            for i, doc in enumerate(docs, 1):
                source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                source_type = doc.metadata.get('source_type', 'ì•Œ ìˆ˜ ì—†ìŒ')
                doc_info = f"[ë¬¸ì„œ {i}] ({source}, {source_type})\n{doc.page_content}\n"
                formatted_docs.append(doc_info)
            
            return "\n".join(formatted_docs)
        
        # retrieverì— k ì„¤ì • ì ìš©
        retriever_with_k = retriever.with_config({"search_kwargs": {"k": k}})
        
        # QA ì²´ì¸ êµ¬ì„±
        qa_chain = (
            {
                "context": retriever_with_k | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt 
            | llm 
            | StrOutputParser()
        )
        
        return qa_chain
        
    except Exception as e:
        st.error(f"âŒ QA ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return None

def stream_response(response_text: str) -> Generator[str, None, None]:
    """ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì¶œë ¥"""
    words = response_text.split()
    for i, word in enumerate(words):
        yield word + " "
        if i % 3 == 0:  # 3ë‹¨ì–´ë§ˆë‹¤ ì ì‹œ ëŒ€ê¸°
            time.sleep(0.05)

def get_rag_response(question: str) -> Optional[str]:
    """RAG ì‹œìŠ¤í…œì—ì„œ ë‹µë³€ ìƒì„±"""
    try:
        if not st.session_state.qa_chain:
            return None
            
        # QA ì²´ì¸ ì‹¤í–‰
        response = st.session_state.qa_chain.invoke(question)
        return response
        
    except Exception as e:
        st.error(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def main():
    """ë©”ì¸ ì•±"""
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    init_session_state()
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # Temperature ìŠ¬ë¼ì´ë”
        temperature = st.slider(
            "ğŸŒ¡ï¸ ì°½ì˜ì„± ì¡°ì ˆ (Temperature)",
            min_value=0.0,
            max_value=1.0,
            value=0.1,
            step=0.1,
            help="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ëœ ë‹µë³€, ë†’ì„ìˆ˜ë¡ ì°½ì˜ì  ë‹µë³€"
        )
        
        # ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ ì„¤ì •
        k = st.number_input(
            "ğŸ“„ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜",
            min_value=1,
            max_value=10,
            value=3,
            help="ê´€ë ¨ ë¬¸ì„œë¥¼ ëª‡ ê°œê¹Œì§€ ê²€ìƒ‰í• ì§€ ì„¤ì •"
        )
        
        st.divider()
        
        # ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ
        if st.session_state.system_ready:
            st.success("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")
        else:
            st.warning("âš ï¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” í•„ìš”")
        
        # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.messages = []
            st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()
        
        st.divider()
        
        # ì•± ì •ë³´
        st.info("""
        **ğŸ¢ HR RAG ì±„íŒ…ë´‡**
        
        ë…¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
        
        **ì£¼ìš” ê¸°ëŠ¥:**
        - ğŸ“š 143ê°œ ë¬¸ì„œ ê¸°ë°˜ ê²€ìƒ‰
        - ğŸ¤– AI ê¸°ë°˜ ë‹µë³€ ìƒì„±  
        - ğŸ’¬ ëŒ€í™” ê¸°ë¡ ìœ ì§€
        - âš™ï¸ ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•
        """)
    
    # ë©”ì¸ í™”ë©´
    st.title("ğŸ¢ HR RAG ì±„íŒ…ë´‡")
    st.caption("ë…¸ë¬´ ì „ë¬¸ê°€ AIê°€ ì—¬ëŸ¬ë¶„ì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤!")
    
    # RAG ì‹œìŠ¤í…œ ë¡œë“œ
    if not load_rag_system(temperature, k):
        st.stop()
    
    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ë…¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # AI ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘..."):
                response = get_rag_response(prompt)
                
            if response:
                # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ì¶œë ¥
                response_placeholder = st.empty()
                streamed_response = ""
                
                for chunk in stream_response(response):
                    streamed_response += chunk
                    response_placeholder.markdown(streamed_response + "â–Œ")
                
                # ìµœì¢… ì‘ë‹µ í‘œì‹œ
                response_placeholder.markdown(response)
                
                # ì‘ë‹µì„ ì„¸ì…˜ì— ì €ì¥
                st.session_state.messages.append({"role": "assistant", "content": response})
            else:
                error_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})

if __name__ == "__main__":
    main() 