# src/preprocessing/qa_chain.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
# from langchain.chains import RetrievalQA # ì´ ë¼ì¸ì€ ë” ì´ìƒ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, ì„ì‹œë¡œ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.
from langchain_community.callbacks import get_openai_callback
import yaml
from langchain_teddynote import logging

# LCEL êµ¬ì¶•ì„ ìœ„í•œ ì¶”ê°€ ì„í¬íŠ¸
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter # ë”•ì…”ë„ˆë¦¬ì—ì„œ íŠ¹ì • í‚¤ì˜ ê°’ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ì‚¬ìš©

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()

# LangSmith ì¶”ì  ì„¤ì •
logging.langsmith("RAG-SAMPLE", set_enable=True)

# retriever.py íŒŒì¼ì—ì„œ initialize_retriever í•¨ìˆ˜ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from retriever import initialize_retriever

def load_prompt_from_yaml(file_path: str):
    """YAML íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config.get("qa_template", "")
    except Exception as e:
        print(f"ì˜¤ë¥˜: YAML íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨ '{file_path}': {e}")
        return ""

def build_qa_chain(retriever, llm_model_name: str = "gpt-4o-mini", llm_options: dict = None):
    """
    ì£¼ì–´ì§„ retrieverì™€ LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆì˜ì‘ë‹µ LCEL ì²´ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
    llm_optionsë¥¼ í†µí•´ LLMì˜ ì¶”ê°€ ë§¤ê°œë³€ìˆ˜ (ì˜ˆ: temperature)ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    print("ğŸš€ ì§ˆì˜ì‘ë‹µ(QA) LCEL ì²´ì¸ êµ¬ì¶• ì‹œì‘...")

    if llm_options is None:
        llm_options = {}

    # ChatOpenAI LLM ì´ˆê¸°í™”
    print(f"LLM ë¡œë“œ ì¤‘: ChatOpenAI (ëª¨ë¸: {llm_model_name}, ì˜µì…˜: {llm_options})")
    try:
        llm = ChatOpenAI(model=llm_model_name, **llm_options)
    except Exception as e:
        print(f"ì˜¤ë¥˜: ChatOpenAI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. OPENAI_API_KEYê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€, ëª¨ë¸ ì´ë¦„('{llm_model_name}')ì´ ìœ íš¨í•œì§€ í™•ì¸í•˜ì„¸ìš”. {e}")
        return None

    # YAML íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))
    prompt_file_path = os.path.join(project_root, 'src', 'prompt', 'qa_prompt.yaml')
    
    qa_template_string = load_prompt_from_yaml(prompt_file_path)

    if not qa_template_string:
        print("âŒ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ ì‹¤íŒ¨. QA ì²´ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    QA_CHAIN_PROMPT = PromptTemplate.from_template(qa_template_string)

    # LCEL ì²´ì¸ êµ¬ì¶• ì‹œì‘
    retrieval_and_pass_through_chain = RunnableParallel({
        "context": itemgetter("question") | retriever,
        "question": itemgetter("question")
    }).with_config(run_name="RetrieveAndPassQuestion")

    answer_generation_chain = (
        QA_CHAIN_PROMPT | llm | StrOutputParser()
    ).with_config(run_name="GenerateAnswer")

    final_rag_chain = (
        retrieval_and_pass_through_chain
        | RunnableParallel({
            "answer": answer_generation_chain,
            "source_documents": itemgetter("context")
        })
    ).with_config(run_name="FinalRAGChain")

    print("âœ… ì§ˆì˜ì‘ë‹µ(QA) LCEL ì²´ì¸ êµ¬ì¶• ì™„ë£Œ!")
    return final_rag_chain

if __name__ == "__main__":
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))

    processed_data_path = os.path.join(project_root, 'data', 'processed', 'documents.pkl')

    print("\n--- ê²€ìƒ‰ê¸°(Retriever) ì´ˆê¸°í™” ì¤‘ ---")
    retriever_instance = initialize_retriever(processed_data_path)

    if retriever_instance:
        print("\n--- ì§ˆì˜ì‘ë‹µ(QA) LCEL ì²´ì¸ êµ¬ì¶• ì¤‘ ---")
        
        # LLM ì˜µì…˜ ì„¤ì • (temperatureë¥¼ 0.0ìœ¼ë¡œ ì„¤ì •)
        llm_custom_options = {
            "temperature": 0.0, # <-- ì´ ê°’ì„ 0.0ìœ¼ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.
        }
        
        qa_chain_instance = build_qa_chain(
            retriever_instance,
            llm_model_name="gpt-4o-mini",
            llm_options=llm_custom_options
        )

        if qa_chain_instance:
            print("\n=== ì§ˆì˜ì‘ë‹µ ì²´ì¸ í…ŒìŠ¤íŠ¸ ===")
            query = "ê¶Œê³ ì‚¬ì§ì„ ë‹¹í–ˆì„ ë•Œ ì‹¤ì—…ê¸‰ì—¬ë¥¼ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?"
            print(f"ì§ˆë¬¸: {query}")

            with get_openai_callback() as cb:
                result = qa_chain_instance.invoke({"question": query})

                print(f"ì´ í† í° ì‚¬ìš©ëŸ‰: {cb.total_tokens}")
                print(f"ì´ ë¹„ìš©: ${cb.total_cost:.4f}")

            print("\n--- ë‹µë³€ ---")
            print(result["answer"])

            print("\n--- ì°¸ì¡° ë¬¸ì„œ ---")
            for i, doc in enumerate(result["source_documents"][:3]):
                print(f"ë¬¸ì„œ {i+1} (Source: {doc.metadata.get('source', 'N/A')}, ID: {doc.metadata.get('column_ID', 'N/A')}):")
                print(f"  {doc.page_content[:200]}...")