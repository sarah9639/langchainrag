import pandas as pd
from src.preprocessing.parsers import parse_excel_for_hr_data
from src.preprocessing.normalizers import clean_text
from langchain_core.documents import Document
import os
import glob
import pickle
from typing import List, Dict, Any

# ìƒˆë¡œìš´ ë¡œë” ì„í¬íŠ¸ (ì„¤ì¹˜ í•„ìš”: pip install pypdf docx2txt unstructured)
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredWordDocumentLoader

def run_preprocessing_pipeline(raw_data_dir: str) -> List[Document]:
    """
    ëª¨ë“  ì›ì‹œ ì—‘ì…€, PDF, Word íŒŒì¼ì„ ì½ê³ , íŒŒì‹± ë° ì •ê·œí™”í•˜ì—¬ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    raw_data_dirì€ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì˜ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•œ ìƒëŒ€ ê²½ë¡œì—¬ì•¼ í•©ë‹ˆë‹¤.
    """
    all_documents = []
    
    print(f"ì „ì²˜ë¦¬ ì‹œì‘: '{raw_data_dir}' ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì¤‘...")
    
    # 1. ì—‘ì…€/CSV íŒŒì¼ ì°¾ê¸° ë° ì²˜ë¦¬
    excel_csv_files = glob.glob(os.path.join(raw_data_dir, "**", "*.xlsx"), recursive=True)
    excel_csv_files.extend(glob.glob(os.path.join(raw_data_dir, "**", "*.xls"), recursive=True))
    excel_csv_files.extend(glob.glob(os.path.join(raw_data_dir, "**", "*.csv"), recursive=True))
    
    print(f"'{raw_data_dir}'ì—ì„œ {len(excel_csv_files)}ê°œì˜ ì—‘ì…€/CSV íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    for file_path in excel_csv_files:
        print(f"ì²˜ë¦¬ ì¤‘ (Excel/CSV): {file_path}")
        try:
            parsed_data_list = parse_excel_for_hr_data(file_path) # parsers.pyì˜ Excel ì²˜ë¦¬ í•¨ìˆ˜ ì‚¬ìš©

            for item in parsed_data_list:
                cleaned_text = clean_text(item['text'])
                if cleaned_text:
                    doc = Document(page_content=cleaned_text, metadata=item['metadata'])
                    all_documents.append(doc)
                else:
                    print(f"ê²½ê³ : {file_path}ì˜ í•œ í–‰ì—ì„œ ì •ê·œí™” í›„ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ë¥¼ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ì˜¤ë¥˜: Excel/CSV íŒŒì¼ '{file_path}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # 2. PDF íŒŒì¼ ì°¾ê¸° ë° ì²˜ë¦¬
    pdf_files = glob.glob(os.path.join(raw_data_dir, "**", "*.pdf"), recursive=True)
    print(f"'{raw_data_dir}'ì—ì„œ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    for file_path in pdf_files:
        print(f"ì²˜ë¦¬ ì¤‘ (PDF): {file_path}")
        try:
            loader = PyPDFLoader(file_path)
            pdf_docs = loader.load()
            for doc in pdf_docs:
                cleaned_text = clean_text(doc.page_content)
                if cleaned_text:
                    # PDFëŠ” ê¸°ë³¸ì ìœ¼ë¡œ page_contentì™€ metadataë¥¼ ê°€ì§‘ë‹ˆë‹¤.
                    # source, page ë“±ì˜ metadataëŠ” loaderê°€ ìë™ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
                    # í•„ìš”í•œ ê²½ìš° ì¶”ê°€ì ì¸ metadataë¥¼ ì—¬ê¸°ì— ë”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                    doc.page_content = cleaned_text
                    doc.metadata["source_file"] = os.path.basename(file_path)
                    doc.metadata["file_path"] = file_path
                    doc.metadata["source_type"] = "pdf"
                    all_documents.append(doc)
                else:
                    print(f"ê²½ê³ : {file_path}ì˜ í•œ í˜ì´ì§€ì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ í˜ì´ì§€ë¥¼ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ì˜¤ë¥˜: PDF íŒŒì¼ '{file_path}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # 3. Word (docx) íŒŒì¼ ì°¾ê¸° ë° ì²˜ë¦¬
    docx_files = glob.glob(os.path.join(raw_data_dir, "**", "*.docx"), recursive=True)
    print(f"'{raw_data_dir}'ì—ì„œ {len(docx_files)}ê°œì˜ Word (docx) íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    for file_path in docx_files:
        print(f"ì²˜ë¦¬ ì¤‘ (Word): {file_path}")
        try:
            # Docx2txtLoaderëŠ” ê°„ë‹¨í•˜ì§€ë§Œ, ë” ê°•ë ¥í•œ íŒŒì‹±ì„ ìœ„í•´ UnstructuredWordDocumentLoader ê¶Œì¥
            # loader = Docx2txtLoader(file_path) # í•„ìš”ì‹œ ì‚¬ìš©
            loader = UnstructuredWordDocumentLoader(file_path) # Unstructured ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”
            word_docs = loader.load()
            for doc in word_docs:
                cleaned_text = clean_text(doc.page_content)
                if cleaned_text:
                    doc.page_content = cleaned_text
                    doc.metadata["source_file"] = os.path.basename(file_path)
                    doc.metadata["file_path"] = file_path
                    doc.metadata["source_type"] = "docx"
                    all_documents.append(doc)
                else:
                    print(f"ê²½ê³ : {file_path}ì˜ í•œ ë¶€ë¶„ì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ ë¶€ë¶„ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ì˜¤ë¥˜: Word íŒŒì¼ '{file_path}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # Fallback: Docx2txtLoader ì‹œë„
            print(f"UnstructuredWordDocumentLoader ì‹¤íŒ¨, Docx2txtLoaderë¡œ ì¬ì‹œë„...")
            try:
                loader = Docx2txtLoader(file_path)
                word_docs = loader.load()
                for doc in word_docs:
                    cleaned_text = clean_text(doc.page_content)
                    if cleaned_text:
                        doc.page_content = cleaned_text
                        doc.metadata["source_file"] = os.path.basename(file_path)
                        doc.metadata["file_path"] = file_path
                        doc.metadata["source_type"] = "docx"
                        all_documents.append(doc)
            except Exception as e2:
                print(f"ì˜¤ë¥˜: Docx2txtLoaderë„ ì‹¤íŒ¨ '{file_path}': {e2}")

    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ. ì´ {len(all_documents)}ê°œì˜ Document ê°ì²´ ìƒì„±.")
    return all_documents

def save_documents_to_pickle(documents: List[Document], output_path: str):
    """
    ìƒì„±ëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ pickle íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    output_pathëŠ” ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì˜ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•œ ìƒëŒ€ ê²½ë¡œì—¬ì•¼ í•©ë‹ˆë‹¤.
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'wb') as f:
        pickle.dump(documents, f)
    print(f"Document ê°ì²´ {len(documents)}ê°œë¥¼ '{output_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

def load_documents_from_pickle(input_path: str) -> List[Document]:
    """
    ì €ì¥ëœ pickle íŒŒì¼ì—ì„œ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        input_path (str): pickle íŒŒì¼ ê²½ë¡œ
        
    Returns:
        List[Document]: ë¡œë“œëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    try:
        with open(input_path, 'rb') as f:
            documents = pickle.load(f)
        print(f"'{input_path}'ì—ì„œ {len(documents)}ê°œì˜ Document ê°ì²´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return documents
    except Exception as e:
        print(f"ì˜¤ë¥˜: pickle íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ '{input_path}': {e}")
        return []

def preview_documents(documents: List[Document], num_samples: int = 3):
    """
    ì²˜ë¦¬ëœ Document ê°ì²´ë“¤ì˜ ìƒ˜í”Œì„ ë¯¸ë¦¬ë³´ê¸°í•©ë‹ˆë‹¤.
    
    Args:
        documents (List[Document]): ë¯¸ë¦¬ë³¼ Document ë¦¬ìŠ¤íŠ¸
        num_samples (int): ë¯¸ë¦¬ë³¼ ìƒ˜í”Œ ìˆ˜
    """
    print(f"\n=== Document ë¯¸ë¦¬ë³´ê¸° (ì´ {len(documents)}ê°œ ì¤‘ {min(num_samples, len(documents))}ê°œ) ===")
    
    for i, doc in enumerate(documents[:num_samples]):
        print(f"\n--- Document {i+1} ---")
        print(f"Content ê¸¸ì´: {len(doc.page_content)} ë¬¸ì")
        print(f"Content ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:200]}...")
        print(f"Metadata: {doc.metadata}")
        print("-" * 50)

if __name__ == "__main__":
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ê³„ì‚°
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))
    
    # ì‹¤ì œ ë°ì´í„° ê²½ë¡œ: "01. consultations/output"
    raw_data_path = os.path.join(project_root, '01. consultations', 'output')
    output_path = os.path.join(project_root, 'data', 'processed', 'documents.pkl')
    
    print(f"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    print(f"ì›ì‹œ ë°ì´í„° ê²½ë¡œ: {raw_data_path}")
    print(f"ì¶œë ¥ íŒŒì¼ ê²½ë¡œ: {output_path}")
    
    # ê²½ë¡œ ì¡´ì¬ í™•ì¸
    if not os.path.exists(raw_data_path):
        print(f"âš ï¸  ê²½ê³ : ì›ì‹œ ë°ì´í„° ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {raw_data_path}")
        print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:")
        print("1. '01. consultations/output' í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€")
        print("2. íŒŒì¼ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€")
        exit(1)
    
    # ì „ì²˜ë¦¬ ì‹¤í–‰
    print("ğŸš€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘!")
    processed_docs = run_preprocessing_pipeline(raw_data_dir=raw_data_path)
    
    if processed_docs:
        # ê²°ê³¼ ì €ì¥
        save_documents_to_pickle(processed_docs, output_path)
        
        # ë¯¸ë¦¬ë³´ê¸°
        preview_documents(processed_docs)
        
        print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“„ ì´ {len(processed_docs)}ê°œì˜ ë¬¸ì„œê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ’¾ ê²°ê³¼ íŒŒì¼: {output_path}")
    else:
        print("âŒ ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ í´ë”ì™€ íŒŒì¼ë“¤ì„ í™•ì¸í•´ì£¼ì„¸ìš”.") 